{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alejandro Paredes, Parameter tuning of BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwAV24NJ-ZSj",
    "outputId": "f707f622-fcd8-4645-fc3f-6beaef291c8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/miniconda3/lib/python3.12/site-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /opt/miniconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: peft in /opt/miniconda3/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: evaluate in /opt/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ilseoplee/.local/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/miniconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/lib/python3.12/site-packages (from datasets) (3.11.0)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/miniconda3/lib/python3.12/site-packages (from peft) (2.4.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/miniconda3/lib/python3.12/site-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/miniconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.12/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "C2gg1Syx-s44"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KfUh_vrS_hbR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Define label maps\n",
    "id2label = {0:\"UNDEFINED\" ,1:\"LEFT\",2:\"RIGHT\",3:\"CENTER\"}\n",
    "label2id = {\"UNDEFINED\": 0, \"LEFT\": 1, \"RIGHT\": 2, \"CENTER\": 3}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=4, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yrj-hSULAi_a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 146718\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df = load_dataset(\"csv\", data_files=\"/Users/ilseoplee/NLPizza_final_project/2017_1.csv\") \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_testvalid = \n",
    "df = df['train'].train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vKd0MpK9BFPv"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ERySJZcQBp9_"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = examples[\"body\"]\n",
    "    labels = examples[\"political_leaning\"]  \n",
    "    \n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512\n",
    "        )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]  \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YmGXgdNQCb7e"
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZgXdBTYUDLz6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693c11197bb74e7c88e6ebc0674297de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/132046 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef06c0c17d2646c2b66999835a9e560f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14672 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 132046\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14672\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = df.map(tokenize_function, batched=True)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IjSOWH6wDNHw"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KJvaBgtkD963"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": accuracy.compute(predictions=predictions\n",
    "                                       , references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6nP1LQquEmt2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model\n",
      "It was good. - LEFT\n",
      "Not a fan, don't recommended - LEFT\n",
      "Better than the first one. - LEFT\n",
      "Women have the right to choose and abortion should be allowed. - LEFT\n"
     ]
    }
   ],
   "source": [
    "text_list = [\"It was good.\", \"Not a fan, don't recommended\",\n",
    "              \"Better than the first one.\", \"Women have the right to choose and abortion should be allowed.\"]\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Untrained model\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)  # Move inputs to the correct device\n",
    "    logits = model(**inputs).logits  # Forward pass\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    print(f'{text} - {id2label[predictions.item()]}')\n",
    "\n",
    "# print(\"Untrained model\")\n",
    "# for text in text_list:\n",
    "#   inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "#   logits = model(inputs).logits\n",
    "#   predictions = torch.argmax(logits)\n",
    "#   print(f'{text} - {id2label[predictions.tolist()]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8xiaVnUaF1Yf"
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type='SEQ_CLS',\n",
    "                         r = 4,\n",
    "                         lora_alpha=32,\n",
    "                         lora_dropout=0.01,\n",
    "                         target_modules = ['q_lin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o4hduUwTGnN5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 630,532 || all params: 67,587,080 || trainable%: 0.9329\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3ORBVjXnGx19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\"+model_checkpoint+\"lora-txt\",\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_epochs,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hmS4TS65IDDV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x0/gkxw33gj58n2ptpf02t8_ncr0000gn/T/ipykernel_66633/1961861285.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gfzk-YnMJL0V"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ff152f1de54fc984fb6cfb2e2a4ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66025 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8446, 'grad_norm': 2.7001771926879883, 'learning_rate': 0.0009924271109428247, 'epoch': 0.04}\n",
      "{'loss': 0.6004, 'grad_norm': 9.18339729309082, 'learning_rate': 0.0009848542218856495, 'epoch': 0.08}\n",
      "{'loss': 0.5425, 'grad_norm': 11.239620208740234, 'learning_rate': 0.0009772813328284742, 'epoch': 0.11}\n",
      "{'loss': 0.5302, 'grad_norm': 3.765162944793701, 'learning_rate': 0.0009697084437712988, 'epoch': 0.15}\n",
      "{'loss': 0.4978, 'grad_norm': 7.315803050994873, 'learning_rate': 0.0009621355547141235, 'epoch': 0.19}\n",
      "{'loss': 0.5099, 'grad_norm': 2.364064931869507, 'learning_rate': 0.0009545626656569481, 'epoch': 0.23}\n",
      "{'loss': 0.4565, 'grad_norm': 2.588992118835449, 'learning_rate': 0.0009469897765997728, 'epoch': 0.27}\n",
      "{'loss': 0.469, 'grad_norm': 5.096480846405029, 'learning_rate': 0.0009394168875425976, 'epoch': 0.3}\n",
      "{'loss': 0.4698, 'grad_norm': 7.594978332519531, 'learning_rate': 0.0009318439984854222, 'epoch': 0.34}\n",
      "{'loss': 0.4569, 'grad_norm': 5.7825446128845215, 'learning_rate': 0.0009242711094282469, 'epoch': 0.38}\n",
      "{'loss': 0.4548, 'grad_norm': 6.436074256896973, 'learning_rate': 0.0009166982203710715, 'epoch': 0.42}\n",
      "{'loss': 0.4551, 'grad_norm': 15.726452827453613, 'learning_rate': 0.0009091253313138963, 'epoch': 0.45}\n",
      "{'loss': 0.4429, 'grad_norm': 16.54352569580078, 'learning_rate': 0.000901552442256721, 'epoch': 0.49}\n",
      "{'loss': 0.451, 'grad_norm': 4.315194606781006, 'learning_rate': 0.0008939795531995456, 'epoch': 0.53}\n",
      "{'loss': 0.4316, 'grad_norm': 0.7392061352729797, 'learning_rate': 0.0008864066641423704, 'epoch': 0.57}\n",
      "{'loss': 0.4434, 'grad_norm': 5.4788713455200195, 'learning_rate': 0.000878833775085195, 'epoch': 0.61}\n",
      "{'loss': 0.4273, 'grad_norm': 6.629356384277344, 'learning_rate': 0.0008712608860280196, 'epoch': 0.64}\n",
      "{'loss': 0.4258, 'grad_norm': 10.987560272216797, 'learning_rate': 0.0008636879969708444, 'epoch': 0.68}\n",
      "{'loss': 0.4401, 'grad_norm': 3.289923906326294, 'learning_rate': 0.0008561151079136691, 'epoch': 0.72}\n",
      "{'loss': 0.4464, 'grad_norm': 5.581179618835449, 'learning_rate': 0.0008485422188564938, 'epoch': 0.76}\n",
      "{'loss': 0.418, 'grad_norm': 9.131034851074219, 'learning_rate': 0.0008409693297993185, 'epoch': 0.8}\n",
      "{'loss': 0.4301, 'grad_norm': 6.297283172607422, 'learning_rate': 0.0008333964407421431, 'epoch': 0.83}\n",
      "{'loss': 0.4381, 'grad_norm': 5.453485488891602, 'learning_rate': 0.0008258235516849678, 'epoch': 0.87}\n",
      "{'loss': 0.4326, 'grad_norm': 1.8949648141860962, 'learning_rate': 0.0008182506626277926, 'epoch': 0.91}\n",
      "{'loss': 0.4335, 'grad_norm': 8.741811752319336, 'learning_rate': 0.0008106777735706173, 'epoch': 0.95}\n",
      "{'loss': 0.405, 'grad_norm': 13.000585556030273, 'learning_rate': 0.0008031048845134419, 'epoch': 0.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e09be13e010459f9727c8b25ce92659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3771231770515442, 'eval_accuracy': {'accuracy': 0.8710468920392584}, 'eval_runtime': 298.3098, 'eval_samples_per_second': 49.184, 'eval_steps_per_second': 4.921, 'epoch': 1.0}\n",
      "{'loss': 0.4396, 'grad_norm': 8.75094985961914, 'learning_rate': 0.0007955319954562665, 'epoch': 1.02}\n",
      "{'loss': 0.4191, 'grad_norm': 5.662125110626221, 'learning_rate': 0.0007879591063990913, 'epoch': 1.06}\n",
      "{'loss': 0.4056, 'grad_norm': 3.3248071670532227, 'learning_rate': 0.0007803862173419159, 'epoch': 1.1}\n",
      "{'loss': 0.4243, 'grad_norm': 20.49200439453125, 'learning_rate': 0.0007728133282847406, 'epoch': 1.14}\n",
      "{'loss': 0.4113, 'grad_norm': 11.759016990661621, 'learning_rate': 0.0007652404392275654, 'epoch': 1.17}\n",
      "{'loss': 0.4125, 'grad_norm': 9.839488983154297, 'learning_rate': 0.00075766755017039, 'epoch': 1.21}\n",
      "{'loss': 0.4089, 'grad_norm': 5.333361625671387, 'learning_rate': 0.0007500946611132147, 'epoch': 1.25}\n",
      "{'loss': 0.4004, 'grad_norm': 9.51548957824707, 'learning_rate': 0.0007425217720560394, 'epoch': 1.29}\n",
      "{'loss': 0.4398, 'grad_norm': 8.943347930908203, 'learning_rate': 0.000734948882998864, 'epoch': 1.33}\n",
      "{'loss': 0.4211, 'grad_norm': 11.688440322875977, 'learning_rate': 0.0007273759939416888, 'epoch': 1.36}\n",
      "{'loss': 0.394, 'grad_norm': 17.41234588623047, 'learning_rate': 0.0007198031048845135, 'epoch': 1.4}\n",
      "{'loss': 0.3988, 'grad_norm': 5.627660274505615, 'learning_rate': 0.0007122302158273382, 'epoch': 1.44}\n",
      "{'loss': 0.4112, 'grad_norm': 3.7049598693847656, 'learning_rate': 0.0007046573267701628, 'epoch': 1.48}\n",
      "{'loss': 0.4205, 'grad_norm': 4.335525989532471, 'learning_rate': 0.0006970844377129875, 'epoch': 1.51}\n",
      "{'loss': 0.4269, 'grad_norm': 7.407427787780762, 'learning_rate': 0.0006895115486558123, 'epoch': 1.55}\n",
      "{'loss': 0.4239, 'grad_norm': 4.416261672973633, 'learning_rate': 0.0006819386595986369, 'epoch': 1.59}\n",
      "{'loss': 0.3868, 'grad_norm': 21.818164825439453, 'learning_rate': 0.0006743657705414615, 'epoch': 1.63}\n",
      "{'loss': 0.4133, 'grad_norm': 10.993179321289062, 'learning_rate': 0.0006667928814842863, 'epoch': 1.67}\n",
      "{'loss': 0.3923, 'grad_norm': 3.070763111114502, 'learning_rate': 0.0006592199924271109, 'epoch': 1.7}\n",
      "{'loss': 0.3792, 'grad_norm': 5.738901615142822, 'learning_rate': 0.0006516471033699356, 'epoch': 1.74}\n",
      "{'loss': 0.3914, 'grad_norm': 3.2593467235565186, 'learning_rate': 0.0006440742143127604, 'epoch': 1.78}\n",
      "{'loss': 0.3964, 'grad_norm': 0.8810759782791138, 'learning_rate': 0.000636501325255585, 'epoch': 1.82}\n",
      "{'loss': 0.3871, 'grad_norm': 9.933631896972656, 'learning_rate': 0.0006289284361984097, 'epoch': 1.86}\n",
      "{'loss': 0.3691, 'grad_norm': 3.903221368789673, 'learning_rate': 0.0006213555471412344, 'epoch': 1.89}\n",
      "{'loss': 0.3828, 'grad_norm': 6.457468509674072, 'learning_rate': 0.000613782658084059, 'epoch': 1.93}\n",
      "{'loss': 0.3877, 'grad_norm': 19.92314338684082, 'learning_rate': 0.0006062097690268838, 'epoch': 1.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df59609259e848dcbcd789c5a5f1b00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3429100215435028, 'eval_accuracy': {'accuracy': 0.8820883315158125}, 'eval_runtime': 309.9586, 'eval_samples_per_second': 47.335, 'eval_steps_per_second': 4.736, 'epoch': 2.0}\n",
      "{'loss': 0.3956, 'grad_norm': 1.027021050453186, 'learning_rate': 0.0005986368799697085, 'epoch': 2.01}\n",
      "{'loss': 0.3551, 'grad_norm': 9.879945755004883, 'learning_rate': 0.0005910639909125332, 'epoch': 2.04}\n",
      "{'loss': 0.3768, 'grad_norm': 3.439880132675171, 'learning_rate': 0.0005834911018553578, 'epoch': 2.08}\n",
      "{'loss': 0.3653, 'grad_norm': 6.963294982910156, 'learning_rate': 0.0005759182127981825, 'epoch': 2.12}\n",
      "{'loss': 0.3783, 'grad_norm': 5.618332386016846, 'learning_rate': 0.0005683453237410072, 'epoch': 2.16}\n",
      "{'loss': 0.3658, 'grad_norm': 8.744894981384277, 'learning_rate': 0.0005607724346838319, 'epoch': 2.2}\n",
      "{'loss': 0.3746, 'grad_norm': 3.110947370529175, 'learning_rate': 0.0005531995456266566, 'epoch': 2.23}\n",
      "{'loss': 0.3796, 'grad_norm': 5.894505977630615, 'learning_rate': 0.0005456266565694813, 'epoch': 2.27}\n",
      "{'loss': 0.3744, 'grad_norm': 7.875442028045654, 'learning_rate': 0.0005380537675123059, 'epoch': 2.31}\n",
      "{'loss': 0.3759, 'grad_norm': 29.62271499633789, 'learning_rate': 0.0005304808784551306, 'epoch': 2.35}\n",
      "{'loss': 0.3497, 'grad_norm': 6.134800910949707, 'learning_rate': 0.0005229079893979554, 'epoch': 2.39}\n",
      "{'loss': 0.3662, 'grad_norm': 16.544483184814453, 'learning_rate': 0.00051533510034078, 'epoch': 2.42}\n",
      "{'loss': 0.3459, 'grad_norm': 2.161447763442993, 'learning_rate': 0.0005077622112836047, 'epoch': 2.46}\n",
      "{'loss': 0.3621, 'grad_norm': 14.708195686340332, 'learning_rate': 0.0005001893222264294, 'epoch': 2.5}\n",
      "{'loss': 0.3776, 'grad_norm': 12.812597274780273, 'learning_rate': 0.0004926164331692541, 'epoch': 2.54}\n",
      "{'loss': 0.3598, 'grad_norm': 4.525390148162842, 'learning_rate': 0.0004850435441120788, 'epoch': 2.57}\n",
      "{'loss': 0.3285, 'grad_norm': 5.74492073059082, 'learning_rate': 0.00047747065505490346, 'epoch': 2.61}\n",
      "{'loss': 0.3549, 'grad_norm': 12.685956001281738, 'learning_rate': 0.00046989776599772813, 'epoch': 2.65}\n",
      "{'loss': 0.356, 'grad_norm': 6.427628517150879, 'learning_rate': 0.0004623248769405528, 'epoch': 2.69}\n",
      "{'loss': 0.3458, 'grad_norm': 8.096437454223633, 'learning_rate': 0.0004547519878833775, 'epoch': 2.73}\n",
      "{'loss': 0.343, 'grad_norm': 3.217128038406372, 'learning_rate': 0.00044717909882620224, 'epoch': 2.76}\n",
      "{'loss': 0.3449, 'grad_norm': 7.965932369232178, 'learning_rate': 0.0004396062097690269, 'epoch': 2.8}\n",
      "{'loss': 0.3324, 'grad_norm': 11.96763801574707, 'learning_rate': 0.00043203332071185157, 'epoch': 2.84}\n",
      "{'loss': 0.3477, 'grad_norm': 7.133875846862793, 'learning_rate': 0.0004244604316546763, 'epoch': 2.88}\n",
      "{'loss': 0.3746, 'grad_norm': 8.779991149902344, 'learning_rate': 0.00041688754259750096, 'epoch': 2.92}\n",
      "{'loss': 0.3233, 'grad_norm': 6.4051690101623535, 'learning_rate': 0.0004093146535403257, 'epoch': 2.95}\n",
      "{'loss': 0.3509, 'grad_norm': 4.999040603637695, 'learning_rate': 0.0004017417644831503, 'epoch': 2.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6019ef9ef84b53972b8354d296e265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29516446590423584, 'eval_accuracy': {'accuracy': 0.8969465648854962}, 'eval_runtime': 302.592, 'eval_samples_per_second': 48.488, 'eval_steps_per_second': 4.851, 'epoch': 3.0}\n",
      "{'loss': 0.3311, 'grad_norm': 0.2548888921737671, 'learning_rate': 0.000394168875425975, 'epoch': 3.03}\n",
      "{'loss': 0.3041, 'grad_norm': 1.0469456911087036, 'learning_rate': 0.00038659598636879973, 'epoch': 3.07}\n",
      "{'loss': 0.3284, 'grad_norm': 2.9848968982696533, 'learning_rate': 0.0003790230973116244, 'epoch': 3.1}\n",
      "{'loss': 0.3259, 'grad_norm': 0.7888076901435852, 'learning_rate': 0.00037145020825444906, 'epoch': 3.14}\n",
      "{'loss': 0.3236, 'grad_norm': 6.392965316772461, 'learning_rate': 0.0003638773191972738, 'epoch': 3.18}\n",
      "{'loss': 0.3173, 'grad_norm': 5.866499423980713, 'learning_rate': 0.00035630443014009845, 'epoch': 3.22}\n",
      "{'loss': 0.3112, 'grad_norm': 11.672548294067383, 'learning_rate': 0.00034873154108292317, 'epoch': 3.26}\n",
      "{'loss': 0.3206, 'grad_norm': 1.3947741985321045, 'learning_rate': 0.0003411586520257478, 'epoch': 3.29}\n",
      "{'loss': 0.3209, 'grad_norm': 1.5705349445343018, 'learning_rate': 0.0003335857629685725, 'epoch': 3.33}\n",
      "{'loss': 0.3063, 'grad_norm': 7.917718410491943, 'learning_rate': 0.0003260128739113972, 'epoch': 3.37}\n",
      "{'loss': 0.3205, 'grad_norm': 4.255771636962891, 'learning_rate': 0.0003184399848542219, 'epoch': 3.41}\n",
      "{'loss': 0.3239, 'grad_norm': 3.423719882965088, 'learning_rate': 0.0003108670957970466, 'epoch': 3.45}\n",
      "{'loss': 0.3156, 'grad_norm': 15.100688934326172, 'learning_rate': 0.0003032942067398713, 'epoch': 3.48}\n",
      "{'loss': 0.3237, 'grad_norm': 5.379032611846924, 'learning_rate': 0.00029572131768269594, 'epoch': 3.52}\n",
      "{'loss': 0.2959, 'grad_norm': 0.5555577874183655, 'learning_rate': 0.00028814842862552066, 'epoch': 3.56}\n",
      "{'loss': 0.3073, 'grad_norm': 10.749110221862793, 'learning_rate': 0.00028057553956834533, 'epoch': 3.6}\n",
      "{'loss': 0.2972, 'grad_norm': 4.572937488555908, 'learning_rate': 0.00027300265051117, 'epoch': 3.63}\n",
      "{'loss': 0.2895, 'grad_norm': 14.281198501586914, 'learning_rate': 0.0002654297614539947, 'epoch': 3.67}\n",
      "{'loss': 0.3086, 'grad_norm': 0.20863492786884308, 'learning_rate': 0.0002578568723968194, 'epoch': 3.71}\n",
      "{'loss': 0.2869, 'grad_norm': 1.2053083181381226, 'learning_rate': 0.0002502839833396441, 'epoch': 3.75}\n",
      "{'loss': 0.2861, 'grad_norm': 4.649848937988281, 'learning_rate': 0.00024271109428246877, 'epoch': 3.79}\n",
      "{'loss': 0.295, 'grad_norm': 0.44491344690322876, 'learning_rate': 0.00023513820522529344, 'epoch': 3.82}\n",
      "{'loss': 0.3003, 'grad_norm': 8.80285358428955, 'learning_rate': 0.00022756531616811816, 'epoch': 3.86}\n",
      "{'loss': 0.284, 'grad_norm': 1.5752969980239868, 'learning_rate': 0.00021999242711094282, 'epoch': 3.9}\n",
      "{'loss': 0.295, 'grad_norm': 0.6969649195671082, 'learning_rate': 0.00021241953805376752, 'epoch': 3.94}\n",
      "{'loss': 0.2897, 'grad_norm': 4.452877044677734, 'learning_rate': 0.00020484664899659218, 'epoch': 3.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff4ace28a8c417198aeee5476a85afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28066781163215637, 'eval_accuracy': {'accuracy': 0.9083287895310797}, 'eval_runtime': 283.7647, 'eval_samples_per_second': 51.705, 'eval_steps_per_second': 5.173, 'epoch': 4.0}\n",
      "{'loss': 0.2605, 'grad_norm': 3.2562201023101807, 'learning_rate': 0.0001972737599394169, 'epoch': 4.01}\n",
      "{'loss': 0.2866, 'grad_norm': 6.792115688323975, 'learning_rate': 0.0001897008708822416, 'epoch': 4.05}\n",
      "{'loss': 0.2637, 'grad_norm': 0.23758991062641144, 'learning_rate': 0.00018212798182506626, 'epoch': 4.09}\n",
      "{'loss': 0.2786, 'grad_norm': 2.8434126377105713, 'learning_rate': 0.00017455509276789096, 'epoch': 4.13}\n",
      "{'loss': 0.2667, 'grad_norm': 5.341547012329102, 'learning_rate': 0.00016698220371071565, 'epoch': 4.17}\n",
      "{'loss': 0.2879, 'grad_norm': 4.266791343688965, 'learning_rate': 0.00015940931465354034, 'epoch': 4.2}\n",
      "{'loss': 0.2701, 'grad_norm': 1.939521074295044, 'learning_rate': 0.000151836425596365, 'epoch': 4.24}\n",
      "{'loss': 0.2493, 'grad_norm': 0.46833840012550354, 'learning_rate': 0.0001442635365391897, 'epoch': 4.28}\n",
      "{'loss': 0.288, 'grad_norm': 1.464335560798645, 'learning_rate': 0.0001366906474820144, 'epoch': 4.32}\n",
      "{'loss': 0.2805, 'grad_norm': 2.5205163955688477, 'learning_rate': 0.0001291177584248391, 'epoch': 4.35}\n",
      "{'loss': 0.2765, 'grad_norm': 0.21555088460445404, 'learning_rate': 0.00012154486936766377, 'epoch': 4.39}\n",
      "{'loss': 0.2535, 'grad_norm': 0.11616695672273636, 'learning_rate': 0.00011397198031048846, 'epoch': 4.43}\n",
      "{'loss': 0.2707, 'grad_norm': 1.278014898300171, 'learning_rate': 0.00010639909125331314, 'epoch': 4.47}\n",
      "{'loss': 0.2646, 'grad_norm': 1.6294682025909424, 'learning_rate': 9.882620219613784e-05, 'epoch': 4.51}\n",
      "{'loss': 0.2676, 'grad_norm': 10.074640274047852, 'learning_rate': 9.125331313896252e-05, 'epoch': 4.54}\n",
      "{'loss': 0.2698, 'grad_norm': 6.275032043457031, 'learning_rate': 8.368042408178721e-05, 'epoch': 4.58}\n",
      "{'loss': 0.253, 'grad_norm': 1.017545223236084, 'learning_rate': 7.610753502461189e-05, 'epoch': 4.62}\n",
      "{'loss': 0.2738, 'grad_norm': 0.0677705779671669, 'learning_rate': 6.853464596743658e-05, 'epoch': 4.66}\n",
      "{'loss': 0.2829, 'grad_norm': 8.23018741607666, 'learning_rate': 6.096175691026127e-05, 'epoch': 4.7}\n",
      "{'loss': 0.2605, 'grad_norm': 4.604710102081299, 'learning_rate': 5.3388867853085956e-05, 'epoch': 4.73}\n",
      "{'loss': 0.2683, 'grad_norm': 5.051339149475098, 'learning_rate': 4.581597879591064e-05, 'epoch': 4.77}\n",
      "{'loss': 0.2674, 'grad_norm': 0.5773504972457886, 'learning_rate': 3.824308973873533e-05, 'epoch': 4.81}\n",
      "{'loss': 0.2504, 'grad_norm': 3.3546910285949707, 'learning_rate': 3.0670200681560016e-05, 'epoch': 4.85}\n",
      "{'loss': 0.249, 'grad_norm': 6.291164875030518, 'learning_rate': 2.3097311624384703e-05, 'epoch': 4.88}\n",
      "{'loss': 0.2559, 'grad_norm': 8.997084617614746, 'learning_rate': 1.552442256720939e-05, 'epoch': 4.92}\n",
      "{'loss': 0.2761, 'grad_norm': 15.997800827026367, 'learning_rate': 7.951533510034078e-06, 'epoch': 4.96}\n",
      "{'loss': 0.2551, 'grad_norm': 3.4847781658172607, 'learning_rate': 3.786444528587656e-07, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47de7c0936074e42ad802c05e4781968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25740399956703186, 'eval_accuracy': {'accuracy': 0.9148037077426391}, 'eval_runtime': 289.8877, 'eval_samples_per_second': 50.613, 'eval_steps_per_second': 5.064, 'epoch': 5.0}\n",
      "{'train_runtime': 34774.366, 'train_samples_per_second': 18.986, 'train_steps_per_second': 1.899, 'train_loss': 0.36235147410475455, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=66025, training_loss=0.36235147410475455, metrics={'train_runtime': 34774.366, 'train_samples_per_second': 18.986, 'train_steps_per_second': 1.899, 'total_flos': 8.874093177643008e+16, 'train_loss': 0.36235147410475455, 'epoch': 5.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "0jn1iHMyJNtM"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrained model predictions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m text_list:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 780 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    309\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "print('Trained model predictions')\n",
    "for text in text_list:\n",
    "  inputs = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "  logits = model(inputs).logits\n",
    "  predictions = torch.max(logits,1).indices\n",
    "\n",
    "  print(f'{text} - {id2label[predictions.tolist()[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQ9UjA-nLGzS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n"
     ]
    }
   ],
   "source": [
    "output_model_file = 'pytorch_distilbert_imbd.bin'\n",
    "output_vocab_file = 'vocab_distilbert_imbd.bin'\n",
    "\n",
    "# Save model\n",
    "model_to_save = model\n",
    "torch.save(model_to_save, output_model_file)\n",
    "\n",
    "# Save tokenizer vocabulary in the current directory\n",
    "tokenizer.save_vocabulary(\".\")  # Current directory\n",
    "\n",
    "# Save model state dictionary\n",
    "torch.save(model.state_dict(), 'trained_model_gral_imbd.pth')\n",
    "\n",
    "print('All files saved')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
